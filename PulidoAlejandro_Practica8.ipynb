{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5468ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.nn import RNN\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed00ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Último</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30/12/2022</th>\n",
       "      <td>10.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/12/2022</th>\n",
       "      <td>10.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/12/2022</th>\n",
       "      <td>10.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/12/2022</th>\n",
       "      <td>10.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23/12/2022</th>\n",
       "      <td>10.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08/01/2010</th>\n",
       "      <td>6.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07/01/2010</th>\n",
       "      <td>6.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06/01/2010</th>\n",
       "      <td>6.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05/01/2010</th>\n",
       "      <td>6.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04/01/2010</th>\n",
       "      <td>6.710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3322 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Último\n",
       "Fecha             \n",
       "30/12/2022  10.751\n",
       "29/12/2022  10.834\n",
       "28/12/2022  10.746\n",
       "27/12/2022  10.900\n",
       "23/12/2022  10.880\n",
       "...            ...\n",
       "08/01/2010   6.700\n",
       "07/01/2010   6.690\n",
       "06/01/2010   6.710\n",
       "05/01/2010   6.740\n",
       "04/01/2010   6.710\n",
       "\n",
       "[3322 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Datos históricos IBE 2010-2022.csv\").iloc[:,:2]\n",
    "data.set_index(\"Fecha\", inplace=True) #inplace=True modifica el DataFrame original en lugar de devolver uno nuevo\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "106f2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=20\n",
    "for i in range(1, n_steps+1):\n",
    "    data[f'Último(t-{i})'] = data['Último'].shift(i)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7abc84ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Último</th>\n",
       "      <th>Último(t-1)</th>\n",
       "      <th>Último(t-2)</th>\n",
       "      <th>Último(t-3)</th>\n",
       "      <th>Último(t-4)</th>\n",
       "      <th>Último(t-5)</th>\n",
       "      <th>Último(t-6)</th>\n",
       "      <th>Último(t-7)</th>\n",
       "      <th>Último(t-8)</th>\n",
       "      <th>Último(t-9)</th>\n",
       "      <th>...</th>\n",
       "      <th>Último(t-11)</th>\n",
       "      <th>Último(t-12)</th>\n",
       "      <th>Último(t-13)</th>\n",
       "      <th>Último(t-14)</th>\n",
       "      <th>Último(t-15)</th>\n",
       "      <th>Último(t-16)</th>\n",
       "      <th>Último(t-17)</th>\n",
       "      <th>Último(t-18)</th>\n",
       "      <th>Último(t-19)</th>\n",
       "      <th>Último(t-20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01/12/2022</th>\n",
       "      <td>10.915</td>\n",
       "      <td>10.850</td>\n",
       "      <td>10.895</td>\n",
       "      <td>10.905</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.910</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>11.055</td>\n",
       "      <td>...</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.810</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.88</td>\n",
       "      <td>10.90</td>\n",
       "      <td>10.746</td>\n",
       "      <td>10.834</td>\n",
       "      <td>10.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/11/2022</th>\n",
       "      <td>10.780</td>\n",
       "      <td>10.915</td>\n",
       "      <td>10.850</td>\n",
       "      <td>10.895</td>\n",
       "      <td>10.905</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.910</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>...</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.810</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.88</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.746</td>\n",
       "      <td>10.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/11/2022</th>\n",
       "      <td>10.655</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.915</td>\n",
       "      <td>10.850</td>\n",
       "      <td>10.895</td>\n",
       "      <td>10.905</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.910</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>...</td>\n",
       "      <td>11.055</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.810</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.880</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/11/2022</th>\n",
       "      <td>10.755</td>\n",
       "      <td>10.655</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.915</td>\n",
       "      <td>10.850</td>\n",
       "      <td>10.895</td>\n",
       "      <td>10.905</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.910</td>\n",
       "      <td>10.995</td>\n",
       "      <td>...</td>\n",
       "      <td>10.995</td>\n",
       "      <td>11.055</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.93</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.880</td>\n",
       "      <td>10.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25/11/2022</th>\n",
       "      <td>10.820</td>\n",
       "      <td>10.755</td>\n",
       "      <td>10.655</td>\n",
       "      <td>10.780</td>\n",
       "      <td>10.915</td>\n",
       "      <td>10.850</td>\n",
       "      <td>10.895</td>\n",
       "      <td>10.905</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.910</td>\n",
       "      <td>...</td>\n",
       "      <td>10.995</td>\n",
       "      <td>10.995</td>\n",
       "      <td>11.055</td>\n",
       "      <td>10.900</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.78</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.930</td>\n",
       "      <td>10.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08/01/2010</th>\n",
       "      <td>6.700</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.580</td>\n",
       "      <td>6.610</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.520</td>\n",
       "      <td>6.420</td>\n",
       "      <td>...</td>\n",
       "      <td>6.290</td>\n",
       "      <td>6.340</td>\n",
       "      <td>6.260</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.20</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.180</td>\n",
       "      <td>5.890</td>\n",
       "      <td>5.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07/01/2010</th>\n",
       "      <td>6.690</td>\n",
       "      <td>6.700</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.580</td>\n",
       "      <td>6.610</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.520</td>\n",
       "      <td>...</td>\n",
       "      <td>6.350</td>\n",
       "      <td>6.290</td>\n",
       "      <td>6.340</td>\n",
       "      <td>6.260</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.20</td>\n",
       "      <td>6.260</td>\n",
       "      <td>6.180</td>\n",
       "      <td>5.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06/01/2010</th>\n",
       "      <td>6.710</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.700</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.580</td>\n",
       "      <td>6.610</td>\n",
       "      <td>6.670</td>\n",
       "      <td>...</td>\n",
       "      <td>6.420</td>\n",
       "      <td>6.350</td>\n",
       "      <td>6.290</td>\n",
       "      <td>6.340</td>\n",
       "      <td>6.260</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.200</td>\n",
       "      <td>6.260</td>\n",
       "      <td>6.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05/01/2010</th>\n",
       "      <td>6.740</td>\n",
       "      <td>6.710</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.700</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.580</td>\n",
       "      <td>6.610</td>\n",
       "      <td>...</td>\n",
       "      <td>6.520</td>\n",
       "      <td>6.420</td>\n",
       "      <td>6.350</td>\n",
       "      <td>6.290</td>\n",
       "      <td>6.340</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.200</td>\n",
       "      <td>6.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04/01/2010</th>\n",
       "      <td>6.710</td>\n",
       "      <td>6.740</td>\n",
       "      <td>6.710</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.700</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.580</td>\n",
       "      <td>...</td>\n",
       "      <td>6.670</td>\n",
       "      <td>6.520</td>\n",
       "      <td>6.420</td>\n",
       "      <td>6.350</td>\n",
       "      <td>6.290</td>\n",
       "      <td>6.34</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3302 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Último  Último(t-1)  Último(t-2)  Último(t-3)  Último(t-4)  \\\n",
       "Fecha                                                                    \n",
       "01/12/2022  10.915       10.850       10.895       10.905       10.920   \n",
       "30/11/2022  10.780       10.915       10.850       10.895       10.905   \n",
       "29/11/2022  10.655       10.780       10.915       10.850       10.895   \n",
       "28/11/2022  10.755       10.655       10.780       10.915       10.850   \n",
       "25/11/2022  10.820       10.755       10.655       10.780       10.915   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "08/01/2010   6.700        6.680        6.690        6.670        6.680   \n",
       "07/01/2010   6.690        6.700        6.680        6.690        6.670   \n",
       "06/01/2010   6.710        6.690        6.700        6.680        6.690   \n",
       "05/01/2010   6.740        6.710        6.690        6.700        6.680   \n",
       "04/01/2010   6.710        6.740        6.710        6.690        6.700   \n",
       "\n",
       "            Último(t-5)  Último(t-6)  Último(t-7)  Último(t-8)  Último(t-9)  \\\n",
       "Fecha                                                                         \n",
       "01/12/2022       10.910       10.995       10.995       10.995       11.055   \n",
       "30/11/2022       10.920       10.910       10.995       10.995       10.995   \n",
       "29/11/2022       10.905       10.920       10.910       10.995       10.995   \n",
       "28/11/2022       10.895       10.905       10.920       10.910       10.995   \n",
       "25/11/2022       10.850       10.895       10.905       10.920       10.910   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "08/01/2010        6.580        6.610        6.670        6.520        6.420   \n",
       "07/01/2010        6.680        6.580        6.610        6.670        6.520   \n",
       "06/01/2010        6.670        6.680        6.580        6.610        6.670   \n",
       "05/01/2010        6.690        6.670        6.680        6.580        6.610   \n",
       "04/01/2010        6.680        6.690        6.670        6.680        6.580   \n",
       "\n",
       "            ...  Último(t-11)  Último(t-12)  Último(t-13)  Último(t-14)  \\\n",
       "Fecha       ...                                                           \n",
       "01/12/2022  ...        10.715        10.780        10.810        10.930   \n",
       "30/11/2022  ...        10.900        10.715        10.780        10.810   \n",
       "29/11/2022  ...        11.055        10.900        10.715        10.780   \n",
       "28/11/2022  ...        10.995        11.055        10.900        10.715   \n",
       "25/11/2022  ...        10.995        10.995        11.055        10.900   \n",
       "...         ...           ...           ...           ...           ...   \n",
       "08/01/2010  ...         6.290         6.340         6.260         6.170   \n",
       "07/01/2010  ...         6.350         6.290         6.340         6.260   \n",
       "06/01/2010  ...         6.420         6.350         6.290         6.340   \n",
       "05/01/2010  ...         6.520         6.420         6.350         6.290   \n",
       "04/01/2010  ...         6.670         6.520         6.420         6.350   \n",
       "\n",
       "            Último(t-15)  Último(t-16)  Último(t-17)  Último(t-18)  \\\n",
       "Fecha                                                                \n",
       "01/12/2022        10.930         10.88         10.90        10.746   \n",
       "30/11/2022        10.930         10.93         10.88        10.900   \n",
       "29/11/2022        10.810         10.93         10.93        10.880   \n",
       "28/11/2022        10.780         10.81         10.93        10.930   \n",
       "25/11/2022        10.715         10.78         10.81        10.930   \n",
       "...                  ...           ...           ...           ...   \n",
       "08/01/2010         6.170          6.20          6.26         6.180   \n",
       "07/01/2010         6.170          6.17          6.20         6.260   \n",
       "06/01/2010         6.260          6.17          6.17         6.200   \n",
       "05/01/2010         6.340          6.26          6.17         6.170   \n",
       "04/01/2010         6.290          6.34          6.26         6.170   \n",
       "\n",
       "            Último(t-19)  Último(t-20)  \n",
       "Fecha                                   \n",
       "01/12/2022        10.834        10.751  \n",
       "30/11/2022        10.746        10.834  \n",
       "29/11/2022        10.900        10.746  \n",
       "28/11/2022        10.880        10.900  \n",
       "25/11/2022        10.930        10.880  \n",
       "...                  ...           ...  \n",
       "08/01/2010         5.890         5.800  \n",
       "07/01/2010         6.180         5.890  \n",
       "06/01/2010         6.260         6.180  \n",
       "05/01/2010         6.200         6.260  \n",
       "04/01/2010         6.170         6.200  \n",
       "\n",
       "[3302 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c34f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data/10 -1 #Por qué escalar de esta forma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "764b63e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0751,  0.0834,  0.0746, ..., -0.382 , -0.374 , -0.38  ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(data.iloc[:,-1])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69291fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0915,  0.085 ,  0.0895, ...,  0.09  ,  0.0746,  0.0834],\n",
       "       [ 0.078 ,  0.0915,  0.085 , ...,  0.088 ,  0.09  ,  0.0746],\n",
       "       [ 0.0655,  0.078 ,  0.0915, ...,  0.093 ,  0.088 ,  0.09  ],\n",
       "       ...,\n",
       "       [-0.329 , -0.331 , -0.33  , ..., -0.383 , -0.38  , -0.374 ],\n",
       "       [-0.326 , -0.329 , -0.331 , ..., -0.383 , -0.383 , -0.38  ],\n",
       "       [-0.329 , -0.326 , -0.329 , ..., -0.374 , -0.383 , -0.383 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array(data.iloc[:,:-1])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42bd3d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3302, 20), (3302,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1794b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensores\n",
    "x_train = torch.unsqueeze(torch.from_numpy(x_train).to(torch.float32),dim=-1)\n",
    "x_test = torch.unsqueeze(torch.from_numpy(x_test).to(torch.float32),dim=-1)\n",
    "y_train = torch.unsqueeze(torch.from_numpy(y_train).to(torch.float32),dim=-1)\n",
    "y_test = torch.unsqueeze(torch.from_numpy(y_test).to(torch.float32),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c510d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aff8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(x_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51377304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layer = nn.RNN(input_size, self.hidden_size, num_layers=2, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, 1)\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(2, batch_size, self.hidden_size)\n",
    "        out, h = self.hidden_layer(x, h0)\n",
    "        output = self.tanh(self.output_layer(out[:, -1, :]))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f969be",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=Elman(input_size=1,hidden_size=5,output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc120abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    modelo.train(True)\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0], batch[1]\n",
    "        output = modelo(x_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        running_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss/len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    modelo.train(False)\n",
    "    running_loss =0.0\n",
    "    \n",
    "    for batch_index, batch in enumerate(test_loader):\n",
    "        x_batch,y_batch = batch[0], batch[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output=modelo(x_batch)\n",
    "            loss= loss_function(output,y_batch)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss /len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab91d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss= []\n",
    "test_loss=[]\n",
    "test_accuracy=[]\n",
    "train_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e13f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    train_loss.append(train_one_epoch())\n",
    "    with torch.no_grad():\n",
    "        predicted_train=modelo(x_train).numpy()\n",
    "        train_accuracy.append(np.mean((torch.abs((y_train - predicted_train)/y_train).numpy()<0.05)))\n",
    "    print(\"Train Loss: {0:.5f}\".format(train_loss[-1]),end=\"\\t\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_test=modelo(x_test).numpy()\n",
    "        test_accuracy.append(np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05)))\n",
    "    \n",
    "    test_loss.append(validate_one_epoch())\n",
    "    print(\"Val Loss: {0:.5f}\".format(test_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(20,20))\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_train=modelo(x_train).numpy()\n",
    "with torch.no_grad():\n",
    "    predicted_test=modelo(x_test).numpy()    \n",
    "    \n",
    "ax[0,0].plot(y_train, label='Actual Close')\n",
    "ax[0,0].plot(predicted_train,label=\"Predicted Close\")\n",
    "ax[0,0].set_xlabel('Dia')\n",
    "ax[0,0].set_ylabel('Cierre')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[0,1].plot(y_test, label='Actual Close')\n",
    "ax[0,1].plot(predicted_test,label=\"Predicted Close\")\n",
    "ax[0,1].set_xlabel('Dia')\n",
    "ax[0,1].set_ylabel('cierre')\n",
    "ax[0,1].legend()\n",
    "ax[0,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "ax[1,0].plot(train_loss)\n",
    "ax[1,0].set_xlabel('Epoch')\n",
    "ax[1,0].set_ylabel('Loss Function')\n",
    "ax[1,0].grid()\n",
    "ax[1,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[1,1].plot(test_loss)\n",
    "ax[1,1].set_xlabel('Epoch')\n",
    "ax[1,1].set_ylabel('Loss Function')\n",
    "ax[1,1].grid()\n",
    "ax[1,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41527015",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "ax[0].plot(train_loss, label='train loss')\n",
    "ax[0].plot(test_loss,label=\"test loss\")\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Loss FUNCTION\")\n",
    "\n",
    "ax[1].plot(train_accuracy, label='train accuracy')\n",
    "ax[1].plot(test_accuracy,label=\"test accuracy\")\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00a812",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layer = nn.GRU(input_size, self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, 1)\n",
    "        self.tanh=nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        out, h = self.hidden_layer(x, h0)\n",
    "        output = self.tanh(self.output_layer(out[:, -1, :]))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=GRU(input_size=1,hidden_size=10,output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea29a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss= []\n",
    "test_loss=[]\n",
    "test_accuracy=[]\n",
    "train_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    train_loss.append(train_one_epoch())\n",
    "    with torch.no_grad():\n",
    "        predicted_train=modelo(x_train).numpy()\n",
    "        train_accuracy.append(np.mean((torch.abs((y_train - predicted_train)/y_train).numpy()<0.05)))\n",
    "    print(\"Train Loss: {0:.5f}\".format(train_loss[-1]),end=\"\\t\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_test=modelo(x_test).numpy()\n",
    "        test_accuracy.append(np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05)))\n",
    "    \n",
    "    test_loss.append(validate_one_epoch())\n",
    "    print(\"Val Loss: {0:.5f}\".format(test_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(20,20))\n",
    "\n",
    "    \n",
    "ax[0,0].plot(y_train, label='Actual Close')\n",
    "ax[0,0].plot(predicted_train,label=\"Predicted Close\")\n",
    "ax[0,0].set_xlabel('Dia')\n",
    "ax[0,0].set_ylabel('Cierre')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[0,1].plot(y_test, label='Actual Close')\n",
    "ax[0,1].plot(predicted_test,label=\"Predicted Close\")\n",
    "ax[0,1].set_xlabel('Dia')\n",
    "ax[0,1].set_ylabel('cierre')\n",
    "ax[0,1].legend()\n",
    "ax[0,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "ax[1,0].plot(train_loss)\n",
    "ax[1,0].set_xlabel('Epoch')\n",
    "ax[1,0].set_ylabel('Loss Function')\n",
    "ax[1,0].grid()\n",
    "ax[1,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[1,1].plot(test_loss)\n",
    "ax[1,1].set_xlabel('Epoch')\n",
    "ax[1,1].set_ylabel('Loss Function')\n",
    "ax[1,1].grid()\n",
    "ax[1,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19121a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "ax[0].plot(train_loss, label='train loss')\n",
    "ax[0].plot(test_loss,label=\"test loss\")\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Loss FUNCTION\")\n",
    "\n",
    "ax[1].plot(train_accuracy, label='train accuracy')\n",
    "ax[1].plot(test_accuracy,label=\"test accuracy\")\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3910c8",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_layer = nn.LSTM(input_size, self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.tanh=nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        out, (h, c) = self.hidden_layer(x, (h0,c0))\n",
    "        output = self.tanh(self.output_layer(out[:, -1, :]))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=LSTM(input_size=1,hidden_size=10,output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2294554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss= []\n",
    "test_loss=[]\n",
    "test_accuracy=[]\n",
    "train_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066554fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    train_loss.append(train_one_epoch())\n",
    "    with torch.no_grad():\n",
    "        predicted_train=modelo(x_train).numpy()\n",
    "        train_accuracy.append(np.mean((torch.abs((y_train - predicted_train)/y_train).numpy()<0.05)))\n",
    "    print(\"Train Loss: {0:.5f}\".format(train_loss[-1]),end=\"\\t\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_test=modelo(x_test).numpy()\n",
    "        test_accuracy.append(np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05)))\n",
    "    \n",
    "    test_loss.append(validate_one_epoch())\n",
    "    print(\"Val Loss: {0:.5f}\".format(test_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d23cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(20,20))\n",
    "\n",
    "    \n",
    "ax[0,0].plot(y_train, label='Actual Close')\n",
    "ax[0,0].plot(predicted_train,label=\"Predicted Close\")\n",
    "ax[0,0].set_xlabel('Dia')\n",
    "ax[0,0].set_ylabel('Cierre')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[0,1].plot(y_test, label='Actual Close')\n",
    "ax[0,1].plot(predicted_test,label=\"Predicted Close\")\n",
    "ax[0,1].set_xlabel('Dia')\n",
    "ax[0,1].set_ylabel('cierre')\n",
    "ax[0,1].legend()\n",
    "ax[0,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "ax[1,0].plot(train_loss)\n",
    "ax[1,0].set_xlabel('Epoch')\n",
    "ax[1,0].set_ylabel('Loss Function')\n",
    "ax[1,0].grid()\n",
    "ax[1,0].set_title(\"TRAIN DATASET\")\n",
    "\n",
    "ax[1,1].plot(test_loss)\n",
    "ax[1,1].set_xlabel('Epoch')\n",
    "ax[1,1].set_ylabel('Loss Function')\n",
    "ax[1,1].grid()\n",
    "ax[1,1].set_title(\"TEST DATASET\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f65790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean((torch.abs((y_test - predicted_test)/y_test).numpy()<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "ax[0].plot(train_loss, label='train loss')\n",
    "ax[0].plot(test_loss,label=\"test loss\")\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Loss FUNCTION\")\n",
    "\n",
    "ax[1].plot(train_accuracy, label='train accuracy')\n",
    "ax[1].plot(test_accuracy,label=\"test accuracy\")\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy evolution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
